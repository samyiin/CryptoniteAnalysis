{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# @title pip install\n",
    "# here to make pip installation to be easy to run at colab\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# @title imports\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ],
   "id": "19f597a979ae874b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# @title load the models\n",
   "id": "6515b6e3c4769162"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# for majority voting, just send the same number for stats.\n",
    "def weighted_ensemble_each_model_has_weight(models_list, clues, stats):\n",
    "    \"\"\"\n",
    "    Predict answers for given samples using ensemble methods with the models in models_list.\n",
    "    each model predict the answer, and the answer with the highest sum of f1 score will be chosen.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    models_list : list of models to predict with ensemble method\n",
    "    \n",
    "    X : ndarray of shape (n_samples)\n",
    "        Input data to predict answers for\n",
    "        \n",
    "    stats : list of shape (n_models) with the F1 scores for each model\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    responses : ndarray of shape (n_samples, )\n",
    "        Predicted answers of given samples\n",
    "    \n",
    "    chosen_models : list of lists\n",
    "        List of lists where each sublist contains the indices of the models that contributed to the final prediction for each sample\n",
    "    \"\"\"\n",
    "    n_samples = clues.shape[0]\n",
    "    \n",
    "    # Initialize a list to store the responses\n",
    "    responses = []\n",
    "    # Initialize a list to store the chosen models for each prediction\n",
    "    chosen_models = []\n",
    "\n",
    "    for i, clue in enumerate(clues):\n",
    "        # Initialize a dictionary to store the weighted scores for each possible prediction\n",
    "        prediction_scores = defaultdict(float)\n",
    "        # Initialize a dictionary to store which models contributed to each prediction\n",
    "        model_contributors = defaultdict(list)\n",
    "        \n",
    "        for model_idx, (model, score) in enumerate(zip(models_list, stats)):\n",
    "            # Predict the output for the ith sample using the current model\n",
    "            # TODO check if the prediction is at this form\n",
    "            prediction = model.predict(clue)[0]\n",
    "            \n",
    "            # Add the weighted F1 score to the prediction's total score\n",
    "            prediction_scores[prediction] += score\n",
    "            # Track which models contributed to this prediction\n",
    "            # TODO maybe add model name if in the model object\n",
    "            model_contributors[prediction].append(model_idx)\n",
    "        \n",
    "        # Select the prediction with the highest weighted score\n",
    "        best_prediction = max(prediction_scores, key=prediction_scores.get)\n",
    "        responses.append(best_prediction)\n",
    "        chosen_models.append(model_contributors[best_prediction])\n",
    "    \n",
    "    return responses, chosen_models"
   ],
   "id": "c689368b52c6d09c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def generate_with_confidence(model, clue):\n",
    "    # todo a function that predicts with a model and returns also the confidence\n",
    "    pass"
   ],
   "id": "d9a23cd53e3b1be6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def averaging_scoring_for_clue(model_list, clue, generating_func_with_score):\n",
    "    word_scores = defaultdict(list)\n",
    "    word_sources = defaultdict(list)\n",
    "    \n",
    "    # Generate words and scores from each LLM\n",
    "    for model_idx, model in enumerate(model_list):\n",
    "        word, score = generating_func_with_score(model, clue)\n",
    "        word_scores[word].append(score)\n",
    "        # TODO maybe add model name if in the model object\n",
    "        word_sources[word].append(model_idx)\n",
    "    \n",
    "    # Calculate average scores\n",
    "    average_scores = {word: sum(scores) / len(scores) for word, scores in word_scores.items()}\n",
    "    \n",
    "    # Select the word with the highest average score\n",
    "    final_word = max(average_scores, key=average_scores.get)\n",
    "    contributing_llms = word_sources[final_word]\n",
    "    \n",
    "    return final_word, contributing_llms\n",
    "\n",
    "def averaging_scoring(model_list, clues, generating_func_with_score):\n",
    "    \"\"\"\n",
    "    Predict answers for given samples using ensemble methods with the models in models_list.\n",
    "    each model predict the answer, and the answer with the highest sum of f1 score will be chosen.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    models_list : list of models to predict with ensemble method\n",
    "    \n",
    "    X : ndarray of shape (n_samples)\n",
    "        Input data to predict answers for\n",
    "        \n",
    "    stats : list of shape (n_models) with the confidence scores for each model\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    responses : ndarray of shape (n_samples, )\n",
    "        Predicted answers of given samples\n",
    "    \n",
    "    chosen_models : list of lists\n",
    "        List of lists where each sublist contains the indices of the models that contributed to the final prediction for each sample\n",
    "    \"\"\"\n",
    "    responses = []\n",
    "    chosen_models = []\n",
    "    for clue in clues:\n",
    "        final_word, contributing_llms = averaging_scoring_for_clue(model_list, clue, generating_func_with_score)\n",
    "        responses.append(final_word)\n",
    "        chosen_models.append(contributing_llms)\n",
    "\n",
    "    return responses, chosen_models\n"
   ],
   "id": "1b614202e360a9fa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
