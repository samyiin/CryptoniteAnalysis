{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOYju3fPIrKBt1ADJRJUG1P"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import os\n","import re\n","import random\n","import json\n","# Mount to google drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Change it to your google drive path where this notebook located.\n","drive_path = '/content/drive/MyDrive/Projects/CryptoniteAnalysis/'\n","os.chdir(drive_path)\n","\n","!pip install openai\n","import openai\n","import google.generativeai as genai\n","import copy\n","\n","!pip install datasets\n","from datasets import load_dataset, load_from_disk\n","\n","def load_dataset_from_disk():\n","    data_dir = 'datasets/cryptonite-official-split/'\n","    train_fp = data_dir + 'cryptonite-train.jsonl'\n","    val_fp = data_dir + 'cryptonite-val.jsonl'\n","    test_fp = data_dir + 'cryptonite-test.jsonl'\n","    datasets = load_dataset('json', data_files={'train': train_fp, 'validation': val_fp, 'test': test_fp})\n","    return datasets\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ygOGo7ezv49V","executionInfo":{"status":"ok","timestamp":1726350822882,"user_tz":-180,"elapsed":25980,"user":{"displayName":"Sam Yiin","userId":"15309663941607315772"}},"outputId":"5eee9740-202c-4b99-fa4f-fa63ec5b0c43"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.45.0)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.5.0)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.1)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n","Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.8)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.3)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n","Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.6)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.8)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"]}]},{"cell_type":"code","source":["# @title GPT Chatbot\n","API_KEY=\"YOUR OPENAI API KEY\"\n","\n","# define the openai interface\n","def try_query_GPT(**request_body):\n","    client = openai.OpenAI(api_key=API_KEY)\n","    response = client.chat.completions.create(**request_body)\n","    return response\n","\n","def accept_gpt_response(response):\n","    res_stop = True\n","    # first check if the response is complete\n","    if not response.choices[0].finish_reason == \"stop\":\n","        res_stop = False\n","\n","    # Other checks in the future\n","    return res_stop\n","\n","def query_GPT(**request_body):\n","    response = try_query_GPT(**request_body)\n","    # if response failed\n","    timeout = 0\n","    while not accept_gpt_response(response):\n","        response = try_query_GPT(**request_body)\n","        timeout += 1\n","        if timeout > 10:\n","            raise Exception(\"Query failed\")\n","    return response.choices[0].message.content\n","\n","default_request_body = {\n","    \"model\": \"gpt-4o-mini\",\n","    \"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}],\n","    \"temperature\": 0.7,\n","}\n","\n","\n","class GPTChatBot:\n","    def __init__(self, initial_request_body=default_request_body):\n","        if \"messages\" not in initial_request_body:\n","            raise ValueError(\"messages not in request_body\")\n","        if \"model\" not in initial_request_body:\n","            raise ValueError(\"model not in request_body\")\n","        self.initial_request_body = copy.deepcopy(initial_request_body)\n","\n","        self.chat_history = self.initial_request_body[\"messages\"]\n","\n","    def chat(self, prompt):\n","        # query ChatGPT, but do not add the conversation to history\n","        temp_request_body = copy.deepcopy(self.initial_request_body)\n","        temp_request_body[\"messages\"].append({\"role\": \"user\", \"content\": prompt})\n","        response = query_GPT(**temp_request_body)\n","        return response\n","\n","    def set_chat_history(self, chat_history):\n","        self.chat_history = chat_history\n","\n","# @title Gemini Chatbot\n","GEMINI_KEY=\"YOUR GEMINI API KEY\"\n","\n","# define the openai interface\n","def try_query_Gemini(**request_body):\n","    model = request_body[\"model\"]\n","    chat = model.start_chat(\n","        history=request_body['history']\n","    )\n","    prompt = request_body[\"prompt\"]\n","    response = chat.send_message(prompt, generation_config=request_body[\"generation_config\"])\n","    return response\n","\n","def accept_Gemini_response(response):\n","    res_stop = True\n","    # first check if the response is complete\n","    if not response._done:\n","        res_stop = False\n","\n","    # Other checks in the future\n","    return res_stop\n","\n","def query_Gemini(**request_body):\n","    response = try_query_Gemini(**request_body)\n","    # if response failed\n","    timeout = 0\n","    while not accept_Gemini_response(response):\n","        response = try_query_Gemini(**request_body)\n","        timeout += 1\n","        if timeout > 10:\n","            raise Exception(\"Query failed\")\n","    return response\n","\n","\n","class GeminiChatBot:\n","    def __init__(self, system_prompt=\"You are a helpful assistant.\", gemini_model=\"gemini-1.5-flash\", temperature=0.7):\n","        genai.configure(api_key=GEMINI_KEY)\n","        self.model = genai.GenerativeModel(model_name=gemini_model, system_instruction=system_prompt)\n","        self.generation_config = genai.types.GenerationConfig(temperature=temperature)\n","        self.chat_history = []\n","\n","\n","\n","\n","    def chat(self, prompt):\n","        '''\n","        for gemini we are not puting a interactive chatbot with history, just zero shot.\n","        No need to add the print feature\n","        '''\n","        request_body = {\n","            \"model\": self.model,\n","            \"generation_config\": self.generation_config,\n","            \"history\" : self.chat_history,\n","            \"prompt\": prompt,\n","        }\n","        response = query_Gemini(**request_body)\n","\n","        return response.text\n","    def set_chat_history(self, chat_history):\n","        self.chat_history = chat_history\n","\n","def load_gpt_chat_bot(gpt_model = \"gpt-4o-2024-08-06\"):\n","    messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n","    request_body = {\n","        \"model\": gpt_model,\n","        \"messages\": messages,\n","        \"temperature\": 0.7,\n","    }\n","    chat_bot = GPTChatBot(request_body)\n","\n","    extractor_system_prompt = \"You are served as a information extractor. You will be given the output of an LLM, and a question, and from the given output, you will extract the information that answers the question. Your output will be linked to a computer program, so you will be accurate and concise.\"\n","\n","    # Load the 4o extractor instead\n","    request_body = {\n","        \"model\": \"gpt-4o-2024-08-06\",   # only the 4o model is good enough\n","        \"messages\": [{\"role\": \"system\", \"content\": extractor_system_prompt}],\n","        \"temperature\": 0.2,\n","    }\n","    information_extractor = GPTChatBot(request_body)\n","    return chat_bot, information_extractor\n","\n","def load_gemini_chat_bot(gemini_model = \"gemini-1.5-pro\"):\n","    chat_bot = GeminiChatBot(system_prompt=\"You are a helpful assistant.\", gemini_model=gemini_model, temperature=0.7)\n","\n","\n","    extractor_system_prompt = \"You are served as a information extractor. You will be given the output of an LLM, and a question, and from the given output, you will extract the information that answers the question. Your output will be linked to a computer program, so you will be accurate and concise.\"\n","\n","\n","    # information_extractor = GeminiChatBot(system_prompt=extractor_system_prompt, gemini_model=gemini_model, temperature=0.2)\n","\n","    # Load the 4o extractor instead\n","    request_body = {\n","        \"model\": \"gpt-4o-2024-08-06\",   # only the 4o model is good enough\n","        \"messages\": [{\"role\": \"system\", \"content\": extractor_system_prompt}],\n","        \"temperature\": 0.2,\n","    }\n","    information_extractor = GPTChatBot(request_body)\n","\n","    return chat_bot, information_extractor"],"metadata":{"id":"14FFpz6Gv_8b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Carefullll Do not change this code!!!!\n","\n","from joblib import Memory\n","\n","# Work around for joblib caching in jupyter notebook \"joblib persistence across sessions/machines\"\n","def cache(mem, module, **mem_kwargs):\n","    # model is the notebook/python file name: Jupyter notebook's name is always changing so we need this work around\n","    def cache_(f):\n","        f.__module__ = module\n","        f.__qualname__ = f.__name__\n","        return mem.cache(f, **mem_kwargs)\n","    # return the cache function that will always create same name for cahce directory\n","    return cache_\n","\n","# Create a memory object with a cache directory\n","memory = Memory(location=\"PromptEngineering/FunctionCache\", verbose=0)\n","\n","@cache(memory, \"AutoStrategySelection\")\n","def naive_QA_solve_puzzle(sample, model, attempt=1):\n","    if \"gemini\" in model:\n","        chat_bot, information_extractor = load_gemini_chat_bot(model)\n","    else:\n","        chat_bot, information_extractor = load_gpt_chat_bot(model)\n","    prompt = f\"Can you solve this problem for me? \\n**Clue**: {sample['clue']}\\n**Orientation**: {sample['orientation']}\"\n","    response = chat_bot.chat(prompt)\n","\n","    # extract information\n","    prompt_extract = f\"Given the output:\\n{response}, What is the answer? I don't need other information.\"\n","    response_extract = information_extractor.chat(prompt_extract)\n","    return response, response_extract"],"metadata":{"id":"gklN6lJPwygF"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":277},"id":"ghGJaobSvx1z","executionInfo":{"status":"ok","timestamp":1726343419707,"user_tz":-180,"elapsed":28799,"user":{"displayName":"Sam Yiin","userId":"15309663941607315772"}},"outputId":"de237928-15dd-4bdd-8f59-e8cdb7d2975d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: gemini-1.5-pro\n"]},{"output_type":"stream","name":"stderr","text":[" 16%|██████████▏                                                   | 33/200 [00:04<00:21,  7.95it/s]"]},{"output_type":"stream","name":"stdout","text":["Failed to solve puzzle 19 with model gemini-1.5-pro\n"]},{"output_type":"stream","name":"stderr","text":[" 61%|█████████████████████████████████████▏                       | 122/200 [00:08<00:06, 11.68it/s]"]},{"output_type":"stream","name":"stdout","text":["Failed to solve puzzle 107 with model gemini-1.5-pro\n"]},{"output_type":"stream","name":"stderr","text":["\r 65%|███████████████████████████████████████▋                     | 130/200 [00:12<00:11,  5.91it/s]"]},{"output_type":"stream","name":"stdout","text":["Failed to solve puzzle 128 with model gemini-1.5-pro\n"]},{"output_type":"stream","name":"stderr","text":[" 74%|████████████████████████████████████████████▊                | 147/200 [00:15<00:08,  6.14it/s]"]},{"output_type":"stream","name":"stdout","text":["Failed to solve puzzle 132 with model gemini-1.5-pro\n"]},{"output_type":"stream","name":"stderr","text":[" 88%|█████████████████████████████████████████████████████▉       | 177/200 [00:19<00:03,  7.26it/s]"]},{"output_type":"stream","name":"stdout","text":["Failed to solve puzzle 162 with model gemini-1.5-pro\n"]},{"output_type":"stream","name":"stderr","text":["100%|█████████████████████████████████████████████████████████████| 200/200 [00:19<00:00, 10.16it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Model: gemini-1.5-flash\n"]},{"output_type":"stream","name":"stderr","text":[" 62%|█████████████████████████████████████▊                       | 124/200 [00:02<00:02, 30.81it/s]"]},{"output_type":"stream","name":"stdout","text":["Failed to solve puzzle 107 with model gemini-1.5-flash\n"]},{"output_type":"stream","name":"stderr","text":[" 72%|███████████████████████████████████████████▌                 | 143/200 [00:03<00:02, 20.33it/s]"]},{"output_type":"stream","name":"stdout","text":["Failed to solve puzzle 128 with model gemini-1.5-flash\n"]},{"output_type":"stream","name":"stderr","text":["100%|█████████████████████████████████████████████████████████████| 200/200 [00:04<00:00, 44.81it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Model: gpt-4o-2024-08-06\n"]},{"output_type":"stream","name":"stderr","text":["100%|█████████████████████████████████████████████████████████████| 200/200 [00:02<00:00, 94.33it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Model: gpt-4o-mini\n"]},{"output_type":"stream","name":"stderr","text":["100%|█████████████████████████████████████████████████████████████| 200/200 [00:02<00:00, 85.47it/s]\n"]}],"source":["datasets = load_dataset_from_disk()\n","gemini_pro = \"gemini-1.5-pro\"\n","gemini_flash = \"gemini-1.5-flash\"\n","gpt_4o = \"gpt-4o-2024-08-06\"\n","gpt_4o_mini = \"gpt-4o-mini\"\n","# gpt_o1 = \"o1-preview-2024-09-12\"\n","# gpt_o1_mini = \"o1-mini-2024-09-12\"\n","from tqdm import tqdm\n","\n","for model in [gemini_pro, gemini_flash, gpt_4o, gpt_4o_mini]:\n","    print(f\"Model: {model}\")\n","    for i in tqdm(range(200), ncols=100):\n","        sample = datasets['test'][i]\n","        try:\n","            response, response_extract = naive_QA_solve_puzzle(sample, model=model, attempt=1)\n","        except:\n","            print(f\"Failed to solve puzzle {i} with model {model}\")\n","            continue\n","        # response, response_extract = naive_QA_solve_puzzle(sample, model=model, attempt=1)"]},{"cell_type":"code","source":["test_size = 200\n","\n","model_score = {'test_size': test_size, gemini_pro: 0, gemini_flash: 0, gpt_4o: 0, gpt_4o_mini: 0}\n","\n","for model in [gemini_pro, gemini_flash, gpt_4o, gpt_4o_mini]:\n","    for i in range(test_size):\n","        sample = datasets['test'][i]\n","        try:\n","            response, response_extract = naive_QA_solve_puzzle(sample, model=model, attempt=1)\n","            if response_extract.strip().lower() == sample['answer'].strip().lower():\n","                model_score[model] += 1\n","        except:\n","            print(f\"Failed to solve puzzle {i} with model {model}\")\n","            continue\n","\n","model_score"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":225},"id":"lucofM7zC5Cv","executionInfo":{"status":"ok","timestamp":1726343466926,"user_tz":-180,"elapsed":24887,"user":{"displayName":"Sam Yiin","userId":"15309663941607315772"}},"outputId":"ed0dfdf7-9450-4038-b378-37bd07c3b929"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Failed to solve puzzle 19 with model gemini-1.5-pro\n","Failed to solve puzzle 107 with model gemini-1.5-pro\n","Failed to solve puzzle 128 with model gemini-1.5-pro\n","Failed to solve puzzle 132 with model gemini-1.5-pro\n","Failed to solve puzzle 162 with model gemini-1.5-pro\n","Failed to solve puzzle 107 with model gemini-1.5-flash\n","Failed to solve puzzle 128 with model gemini-1.5-flash\n"]},{"output_type":"execute_result","data":{"text/plain":["{'test_size': 200,\n"," 'gemini-1.5-pro': 12,\n"," 'gemini-1.5-flash': 14,\n"," 'gpt-4o-2024-08-06': 46,\n"," 'gpt-4o-mini': 19}"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["# Results we got from website querying o1"],"metadata":{"id":"w_sqm1_Lhy3e"}},{"cell_type":"code","source":["answers = []\n","for i in range(10):\n","    sample = datasets['test'][i]\n","    prompt = f\"Can you solve this cryptic crossword puzzle for me? \\n**Clue**: {sample['clue']}\\n**Orientation**: {sample['orientation']}\\n\"\n","    print(prompt)\n","    print(f\"Answer: {sample['answer']}\\n\")\n","    answers.append(sample['answer'])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U2wGkeizjp-U","executionInfo":{"status":"ok","timestamp":1726353443606,"user_tz":-180,"elapsed":301,"user":{"displayName":"Sam Yiin","userId":"15309663941607315772"}},"outputId":"7aea035f-aef4-4f63-df1d-dc491cf1f3f5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Can you solve this cryptic crossword puzzle for me? \n","**Clue**: solution woman had found is a turning point (9)\n","**Orientation**: across\n","\n","Answer: watershed\n","\n","Can you solve this cryptic crossword puzzle for me? \n","**Clue**: airborne soldier given particular protection from fire (7)\n","**Orientation**: across\n","\n","Answer: parapet\n","\n","Can you solve this cryptic crossword puzzle for me? \n","**Clue**: closely follow artist, leaving son to pontificate (9)\n","**Orientation**: down\n","\n","Answer: dogmatise\n","\n","Can you solve this cryptic crossword puzzle for me? \n","**Clue**: catholic a vulgar american woman resented (5,6)\n","**Orientation**: down\n","\n","Answer: broad minded\n","\n","Can you solve this cryptic crossword puzzle for me? \n","**Clue**: see newly planted wood falter in this? (5,5)\n","**Orientation**: across\n","\n","Answer: flood water\n","\n","Can you solve this cryptic crossword puzzle for me? \n","**Clue**: the majority will get the next clue! (8)\n","**Orientation**: across\n","\n","Answer: eighteen\n","\n","Can you solve this cryptic crossword puzzle for me? \n","**Clue**: high field put under cultivation (8)\n","**Orientation**: across\n","\n","Answer: uplifted\n","\n","Can you solve this cryptic crossword puzzle for me? \n","**Clue**: player is a bit too boisterous (6)\n","**Orientation**: across\n","\n","Answer: oboist\n","\n","Can you solve this cryptic crossword puzzle for me? \n","**Clue**: a text is accepted in error (5)\n","**Orientation**: down\n","\n","Answer: amiss\n","\n","Can you solve this cryptic crossword puzzle for me? \n","**Clue**: alas, not how fairy stories are supposed to end (9)\n","**Orientation**: down\n","\n","Answer: unhappily\n","\n"]}]},{"cell_type":"code","source":["answers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CaVuJWZfry3k","executionInfo":{"status":"ok","timestamp":1726353455025,"user_tz":-180,"elapsed":302,"user":{"displayName":"Sam Yiin","userId":"15309663941607315772"}},"outputId":"5defffbc-e25f-46c1-e420-f4732176f44b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['watershed',\n"," 'parapet',\n"," 'dogmatise',\n"," 'broad minded',\n"," 'flood water',\n"," 'eighteen',\n"," 'uplifted',\n"," 'oboist',\n"," 'amiss',\n"," 'unhappily']"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["o1_answers = ['WATERSHED', 'PARAPET', 'SERMONISE', 'Grace Mugabe', 'BLIND PANIC', 'MAJORITY', 'UPLIFTED', 'OBOIST', 'EXIST', 'UNHAPPILY']   # 4/10\n","o1_mini_answers = ['WATERSHED', 'PARAPET', 'BLOVIATE', 'Broad Church ', 'TREES LEAN', 'greater', 'TOPSOIL', 'RACKET', 'AMISS', 'HAPPYEND']      # 3/10"],"metadata":{"id":"rnBih3djr0yw"},"execution_count":null,"outputs":[]}]}