{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPE3IZ6BRVaWkjMuGiLbHIc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Load the Context"],"metadata":{"id":"iWPXuatuSAon"}},{"cell_type":"code","source":["import os\n","import re\n","import random\n","import json\n","# Mount to google drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Change it to your google drive path where this notebook located.\n","drive_path = '/content/drive/MyDrive/Projects/CryptoniteAnalysis/'\n","os.chdir(drive_path)\n","\n","!pip install openai\n","import openai\n","import google.generativeai as genai\n","import copy\n","\n","!pip install datasets\n","from datasets import load_dataset, load_from_disk\n","\n","def load_dataset_from_disk():\n","    data_dir = 'datasets/cryptonite-official-split/'\n","    train_fp = data_dir + 'cryptonite-train.jsonl'\n","    val_fp = data_dir + 'cryptonite-val.jsonl'\n","    test_fp = data_dir + 'cryptonite-test.jsonl'\n","    datasets = load_dataset('json', data_files={'train': train_fp, 'validation': val_fp, 'test': test_fp})\n","    return datasets\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5gboTuWtTPIQ","executionInfo":{"status":"ok","timestamp":1726355781661,"user_tz":-180,"elapsed":21556,"user":{"displayName":"Sam Yiin","userId":"15309663941607315772"}},"outputId":"9e7a0b32-fda0-4cc7-fd41-0888bdf145d6"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.45.0)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.5.0)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.1)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n","Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.8)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.3)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n","Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.6)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.8)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"]}]},{"cell_type":"markdown","source":["# LLM ChatBots"],"metadata":{"id":"yCuL2twtTqg5"}},{"cell_type":"code","source":["# @title GPT Chatbot\n","API_KEY=\"YOUR OPENAI API KEY\"\n","\n","# define the openai interface\n","def try_query_GPT(**request_body):\n","    client = openai.OpenAI(api_key=API_KEY)\n","    response = client.chat.completions.create(**request_body)\n","    return response\n","\n","def accept_gpt_response(response):\n","    res_stop = True\n","    # first check if the response is complete\n","    if not response.choices[0].finish_reason == \"stop\":\n","        res_stop = False\n","\n","    # Other checks in the future\n","    return res_stop\n","\n","def query_GPT(**request_body):\n","    response = try_query_GPT(**request_body)\n","    # if response failed\n","    timeout = 0\n","    while not accept_gpt_response(response):\n","        response = try_query_GPT(**request_body)\n","        timeout += 1\n","        if timeout > 10:\n","            raise Exception(\"Query failed\")\n","    return response.choices[0].message.content\n","\n","default_request_body = {\n","    \"model\": \"gpt-4o-mini\",\n","    \"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}],\n","    \"temperature\": 0.7,\n","}\n","\n","\n","class GPTChatBot:\n","    def __init__(self, initial_request_body=default_request_body):\n","        if \"messages\" not in initial_request_body:\n","            raise ValueError(\"messages not in request_body\")\n","        if \"model\" not in initial_request_body:\n","            raise ValueError(\"model not in request_body\")\n","        self.initial_request_body = copy.deepcopy(initial_request_body)\n","\n","        self.chat_history = self.initial_request_body[\"messages\"]\n","\n","    def chat(self, prompt):\n","        # query ChatGPT, but do not add the conversation to history\n","        temp_request_body = copy.deepcopy(self.initial_request_body)\n","        temp_request_body[\"messages\"].append({\"role\": \"user\", \"content\": prompt})\n","        response = query_GPT(**temp_request_body)\n","        return response\n","\n","    def set_chat_history(self, chat_history):\n","        self.chat_history = chat_history\n"],"metadata":{"id":"CpgdGfDaTvr-","executionInfo":{"status":"ok","timestamp":1726355793972,"user_tz":-180,"elapsed":310,"user":{"displayName":"Sam Yiin","userId":"15309663941607315772"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# @title Gemini Chatbot\n","GEMINI_KEY=\"YOUR GEMINI API KEY\"\n","\n","# define the openai interface\n","def try_query_Gemini(**request_body):\n","    model = request_body[\"model\"]\n","    chat = model.start_chat(\n","        history=request_body['history']\n","    )\n","    prompt = request_body[\"prompt\"]\n","    response = chat.send_message(prompt, generation_config=request_body[\"generation_config\"])\n","    return response\n","\n","def accept_Gemini_response(response):\n","    res_stop = True\n","    # first check if the response is complete\n","    if not response._done:\n","        res_stop = False\n","\n","    # Other checks in the future\n","    return res_stop\n","\n","def query_Gemini(**request_body):\n","    response = try_query_Gemini(**request_body)\n","    # if response failed\n","    timeout = 0\n","    while not accept_Gemini_response(response):\n","        response = try_query_Gemini(**request_body)\n","        timeout += 1\n","        if timeout > 10:\n","            raise Exception(\"Query failed\")\n","    return response\n","\n","\n","class GeminiChatBot:\n","    def __init__(self, system_prompt=\"You are a helpful assistant.\", gemini_model=\"gemini-1.5-flash\", temperature=0.7):\n","        genai.configure(api_key=GEMINI_KEY)\n","        self.model = genai.GenerativeModel(model_name=gemini_model, system_instruction=system_prompt)\n","        self.generation_config = genai.types.GenerationConfig(temperature=temperature)\n","        self.chat_history = []\n","\n","\n","\n","\n","    def chat(self, prompt):\n","        '''\n","        for gemini we are not puting a interactive chatbot with history, just zero shot.\n","        No need to add the print feature\n","        '''\n","        request_body = {\n","            \"model\": self.model,\n","            \"generation_config\": self.generation_config,\n","            \"history\" : self.chat_history,\n","            \"prompt\": prompt,\n","        }\n","        response = query_Gemini(**request_body)\n","\n","        return response.text\n","    def set_chat_history(self, chat_history):\n","        self.chat_history = chat_history\n","\n"],"metadata":{"id":"r7FVbuqIUE7M","executionInfo":{"status":"ok","timestamp":1726355796837,"user_tz":-180,"elapsed":300,"user":{"displayName":"Sam Yiin","userId":"15309663941607315772"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# In context learning"],"metadata":{"id":"Da7TWopGVCD9"}},{"cell_type":"markdown","source":["## Load Context to Chatbots"],"metadata":{"id":"wcBh0_eKfjeI"}},{"cell_type":"code","source":["with open('PromptEngineering/InContextLearningExamples/SystemInstructions.md', 'r') as file:\n","    # Read the content of the file\n","    solver_system_prompt = file.read()\n","data = []\n","with open('PromptEngineering/InContextLearningExamples/ProcessedExamples.jsonl', 'r') as f:\n","    for line in f:\n","        # Convert each line from JSON string to a dictionary and append to the list\n","        data.append(json.loads(line.strip()))\n","\n","import random\n","\n","def load_gpt_chat_bot(gpt_model = \"gpt-4o-2024-08-06\"):\n","    messages = [{\"role\": \"system\", \"content\": solver_system_prompt}]\n","    random_numbers = random.sample(range(len(data)), 30)\n","    for i in random_numbers:\n","        example = data[i]\n","        messages.append({\"role\": \"user\", \"content\": example[\"input\"]})\n","        messages.append({\"role\": \"assistant\", \"content\": example[\"explanation\"]})\n","    request_body = {\n","        \"model\": gpt_model,\n","        \"messages\": messages,\n","        \"temperature\": 0.7,\n","    }\n","    chat_bot = GPTChatBot(request_body)\n","\n","    extractor_system_prompt = \"You are served as a information extractor. You will be given the output of an LLM, and a question, and from the given output, you will extract the information that answers the question. Your output will be linked to a computer program, so you will be accurate and concise.\"\n","\n","    # Load the 4o extractor instead\n","    request_body = {\n","        \"model\": \"gpt-4o-2024-08-06\",   # only the 4o model is good enough\n","        \"messages\": [{\"role\": \"system\", \"content\": extractor_system_prompt}],\n","        \"temperature\": 0.2,\n","    }\n","    information_extractor = GPTChatBot(request_body)\n","    return chat_bot, information_extractor\n","\n","\n","def load_gemini_chat_bot(gemini_model = \"gemini-1.5-pro\"):\n","    history = []\n","    random_numbers = random.sample(range(len(data)), 30)\n","    for i in random_numbers:\n","        example = data[i]\n","        history.append({\"role\": \"user\", \"parts\": example[\"input\"]})\n","        history.append({\"role\": \"model\", \"parts\": example[\"explanation\"]})\n","    chat_bot = GeminiChatBot(system_prompt=solver_system_prompt, gemini_model=gemini_model, temperature=0.7)\n","    chat_bot.set_chat_history(history)\n","\n","\n","    extractor_system_prompt = \"You are served as a information extractor. You will be given the output of an LLM, and a question, and from the given output, you will extract the information that answers the question. Your output will be linked to a computer program, so you will be accurate and concise.\"\n","\n","\n","    # information_extractor = GeminiChatBot(system_prompt=extractor_system_prompt, gemini_model=gemini_model, temperature=0.2)\n","\n","    # Load the 4o extractor instead\n","    request_body = {\n","        \"model\": \"gpt-4o-2024-08-06\",   # only the 4o model is good enough\n","        \"messages\": [{\"role\": \"system\", \"content\": extractor_system_prompt}],\n","        \"temperature\": 0.2,\n","    }\n","    information_extractor = GPTChatBot(request_body)\n","\n","    return chat_bot, information_extractor"],"metadata":{"id":"ozkMj0mNVEXt","executionInfo":{"status":"ok","timestamp":1726355799853,"user_tz":-180,"elapsed":323,"user":{"displayName":"Sam Yiin","userId":"15309663941607315772"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["## Load the dataset"],"metadata":{"id":"ZQL402AxfoCl"}},{"cell_type":"code","source":["datasets = load_dataset_from_disk()"],"metadata":{"id":"VtoY6s2jfqVb","executionInfo":{"status":"ok","timestamp":1726355802915,"user_tz":-180,"elapsed":360,"user":{"displayName":"Sam Yiin","userId":"15309663941607315772"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Carefullll Do not change this code!!!!\n","\n","from joblib import Memory\n","\n","# Work around for joblib caching in jupyter notebook \"joblib persistence across sessions/machines\"\n","def cache(mem, module, **mem_kwargs):\n","    # model is the notebook/python file name: Jupyter notebook's name is always changing so we need this work around\n","    def cache_(f):\n","        f.__module__ = module\n","        f.__qualname__ = f.__name__\n","        return mem.cache(f, **mem_kwargs)\n","    # return the cache function that will always create same name for cahce directory\n","    return cache_\n","\n","# Create a memory object with a cache directory\n","memory = Memory(location=\"PromptEngineering/FunctionCache\", verbose=0)\n","\n","@cache(memory, \"AutoStrategySelection\")\n","def solve_puzzle(sample, model, attempt=1):\n","    if \"gemini\" in model:\n","        chat_bot, information_extractor = load_gemini_chat_bot(model)\n","    else:\n","        chat_bot, information_extractor = load_gpt_chat_bot(model)\n","    prompt = f\"**Clue**: {sample['clue']}\\n**Orientation**: {sample['orientation']}\"\n","    response = chat_bot.chat(prompt)\n","\n","    # extract information\n","    prompt_extract = f\"Given the output:\\n{response}, What is the answer? I don't need other information.\"\n","    response_extract = information_extractor.chat(prompt_extract)\n","    return response, response_extract"],"metadata":{"id":"sOk0EL9ohJMy","executionInfo":{"status":"ok","timestamp":1726355804480,"user_tz":-180,"elapsed":319,"user":{"displayName":"Sam Yiin","userId":"15309663941607315772"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["gemini_pro = \"gemini-1.5-pro\"\n","gemini_flash = \"gemini-1.5-flash\"\n","gpt_4o = \"gpt-4o-2024-08-06\"\n","gpt_4o_mini = \"gpt-4o-mini\"\n","from tqdm import tqdm\n","\n","for model in [gemini_pro, gemini_flash, gpt_4o, gpt_4o_mini]:\n","    print(f\"Model: {model}\")\n","    for i in tqdm(range(200), ncols=100):\n","        sample = datasets['test'][i]\n","        try:\n","            response, response_extract = solve_puzzle(sample, model=model, attempt=1)\n","        except:\n","            print(f\"Failed to solve puzzle {i} with model {model}\")\n","            continue\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":233},"id":"nuEKk5QHix-d","executionInfo":{"status":"ok","timestamp":1726355856146,"user_tz":-180,"elapsed":48726,"user":{"displayName":"Sam Yiin","userId":"15309663941607315772"}},"outputId":"1011eee4-8b52-479b-8b1b-2324f42df5e5"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: gemini-1.5-pro\n"]},{"output_type":"stream","name":"stderr","text":[" 60%|████████████████████████████████████▌                        | 120/200 [00:14<00:25,  3.19it/s]"]},{"output_type":"stream","name":"stdout","text":["Failed to solve puzzle 107 with model gemini-1.5-pro\n"]},{"output_type":"stream","name":"stderr","text":[" 69%|██████████████████████████████████████████                   | 138/200 [00:34<00:38,  1.60it/s]"]},{"output_type":"stream","name":"stdout","text":["Failed to solve puzzle 128 with model gemini-1.5-pro\n"]},{"output_type":"stream","name":"stderr","text":["100%|█████████████████████████████████████████████████████████████| 200/200 [00:35<00:00,  5.66it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Model: gemini-1.5-flash\n"]},{"output_type":"stream","name":"stderr","text":[" 70%|███████████████████████████████████████████                  | 141/200 [00:06<00:08,  7.10it/s]"]},{"output_type":"stream","name":"stdout","text":["Failed to solve puzzle 128 with model gemini-1.5-flash\n"]},{"output_type":"stream","name":"stderr","text":["100%|█████████████████████████████████████████████████████████████| 200/200 [00:07<00:00, 27.16it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Model: gpt-4o-2024-08-06\n"]},{"output_type":"stream","name":"stderr","text":["100%|█████████████████████████████████████████████████████████████| 200/200 [00:02<00:00, 71.24it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Model: gpt-4o-mini\n"]},{"output_type":"stream","name":"stderr","text":["100%|█████████████████████████████████████████████████████████████| 200/200 [00:02<00:00, 68.87it/s]\n"]}]},{"cell_type":"code","source":["# @title test without reflection\n","test_size = 200\n","\n","model_score = {'test_size': test_size, gemini_pro: 0, gemini_flash: 0, gpt_4o: 0, gpt_4o_mini: 0}\n","\n","for model in [gemini_pro, gemini_flash, gpt_4o, gpt_4o_mini]:\n","    for i in range(test_size):\n","        sample = datasets['test'][i]\n","        try:\n","            response, response_extract = solve_puzzle(sample, model=model, attempt=1)\n","            if response_extract.strip().lower() == sample['answer'].strip().lower():\n","                model_score[model] += 1\n","        except:\n","            print(f\"Failed to solve puzzle {i} with model {model}\")\n","            continue\n","\n","model_score"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":160},"id":"0lCBYWZ3aAot","executionInfo":{"status":"ok","timestamp":1726355920269,"user_tz":-180,"elapsed":34482,"user":{"displayName":"Sam Yiin","userId":"15309663941607315772"}},"outputId":"7c354edd-d8b0-462e-d0d4-0e280bc6be6c"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Failed to solve puzzle 107 with model gemini-1.5-pro\n","Failed to solve puzzle 128 with model gemini-1.5-pro\n","Failed to solve puzzle 128 with model gemini-1.5-flash\n"]},{"output_type":"execute_result","data":{"text/plain":["{'test_size': 200,\n"," 'gemini-1.5-pro': 29,\n"," 'gemini-1.5-flash': 12,\n"," 'gpt-4o-2024-08-06': 43,\n"," 'gpt-4o-mini': 16}"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["@cache(memory, \"AutoStrategySelection\")\n","def solve_puzzle_reflection(sample, model, attempt=1):\n","    if \"gemini\" in model:\n","        chat_bot, information_extractor = load_gemini_chat_bot(model)\n","        content_name = 'parts'\n","        model_role_name = 'model'\n","    else:\n","        chat_bot, information_extractor = load_gpt_chat_bot(model)\n","        content_name = 'content'\n","        model_role_name = 'assistant'\n","    prompt = f\"**Clue**: {sample['clue']}\\n**Orientation**: {sample['orientation']}\"\n","    response = chat_bot.chat(prompt)\n","\n","    # Now reflect on previous answer, and try again\n","    chat_bot.chat_history.append({\"role\": \"user\", content_name: prompt})\n","    chat_bot.chat_history.append({\"role\": model_role_name, content_name: response})\n","    prompt = f\"Now look at your previous response, do you think your answer is correct? Does the answer fits the enumeration? Does the answer fits the definition? If not, then try again to solve this problem, maybe try another interpretation on what wordplay should you preform, and select new reasoning steps to follow.  \"\n","    response = chat_bot.chat(prompt)\n","\n","    # extract information\n","    prompt_extract = f\"Given the output:\\n{response}, What is the answer? I don't need other information.\"\n","    response_extract = information_extractor.chat(prompt_extract)\n","    return response, response_extract"],"metadata":{"id":"f_9vbIBRG4-a","executionInfo":{"status":"ok","timestamp":1726355996976,"user_tz":-180,"elapsed":345,"user":{"displayName":"Sam Yiin","userId":"15309663941607315772"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["gemini_pro = \"gemini-1.5-pro\"\n","gemini_flash = \"gemini-1.5-flash\"\n","gpt_4o = \"gpt-4o-2024-08-06\"\n","gpt_4o_mini = \"gpt-4o-mini\"\n","from tqdm import tqdm\n","\n","for model in [ gemini_pro, gemini_flash, gpt_4o, gpt_4o_mini]:\n","    print(f\"Model: {model}\")\n","    for i in tqdm(range(200), ncols=100):\n","        sample = datasets['test'][i]\n","        try:\n","            response, response_extract = solve_puzzle_reflection(sample, model=model, attempt=1)\n","        except:\n","            print(f\"Failed to solve puzzle {i} with model {model}\")\n","            continue"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":251},"id":"UB-9GjRNIpfe","executionInfo":{"status":"ok","timestamp":1726357344614,"user_tz":-180,"elapsed":89266,"user":{"displayName":"Sam Yiin","userId":"15309663941607315772"}},"outputId":"ff094104-5ddc-44f9-aa04-78d4b7e6822e"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: gemini-1.5-pro\n"]},{"output_type":"stream","name":"stderr","text":[" 58%|███████████████████████████████████▍                         | 116/200 [00:32<00:28,  2.91it/s]"]},{"output_type":"stream","name":"stdout","text":["Failed to solve puzzle 107 with model gemini-1.5-pro\n"]},{"output_type":"stream","name":"stderr","text":[" 68%|█████████████████████████████████████████▊                   | 137/200 [00:49<00:38,  1.64it/s]"]},{"output_type":"stream","name":"stdout","text":["Failed to solve puzzle 128 with model gemini-1.5-pro\n"]},{"output_type":"stream","name":"stderr","text":["100%|█████████████████████████████████████████████████████████████| 200/200 [00:50<00:00,  3.99it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Model: gemini-1.5-flash\n"]},{"output_type":"stream","name":"stderr","text":[" 55%|█████████████████████████████████▌                           | 110/200 [00:19<00:32,  2.75it/s]"]},{"output_type":"stream","name":"stdout","text":["Failed to solve puzzle 107 with model gemini-1.5-flash\n"]},{"output_type":"stream","name":"stderr","text":[" 69%|██████████████████████████████████████████                   | 138/200 [00:32<00:21,  2.93it/s]"]},{"output_type":"stream","name":"stdout","text":["Failed to solve puzzle 128 with model gemini-1.5-flash\n"]},{"output_type":"stream","name":"stderr","text":["100%|█████████████████████████████████████████████████████████████| 200/200 [00:33<00:00,  6.05it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Model: gpt-4o-2024-08-06\n"]},{"output_type":"stream","name":"stderr","text":["100%|█████████████████████████████████████████████████████████████| 200/200 [00:03<00:00, 65.59it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Model: gpt-4o-mini\n"]},{"output_type":"stream","name":"stderr","text":["100%|█████████████████████████████████████████████████████████████| 200/200 [00:02<00:00, 75.05it/s]\n"]}]},{"cell_type":"code","source":["# @title Test with reflection\n","test_size = 200\n","\n","model_score = {'test_size': test_size, gemini_pro: 0, gemini_flash: 0, gpt_4o: 0, gpt_4o_mini: 0}\n","\n","for model in [gemini_pro, gemini_flash, gpt_4o, gpt_4o_mini]:\n","    for i in range(test_size):\n","        sample = datasets['test'][i]\n","        try:\n","            response1, response_extract1 = solve_puzzle(sample, model=model, attempt=1)\n","            response2, response_extract2 = solve_puzzle_reflection(sample, model=model, attempt=1)\n","            if response_extract1.strip().lower() == sample['answer'].strip().lower() or response_extract2.strip().lower() == sample['answer'].strip().lower():\n","                model_score[model] += 1\n","        except:\n","            print(f\"Failed to solve puzzle {i} with model {model}\")\n","            continue\n","\n","model_score"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":178},"id":"-xIPsSty2l20","executionInfo":{"status":"ok","timestamp":1726358013750,"user_tz":-180,"elapsed":54558,"user":{"displayName":"Sam Yiin","userId":"15309663941607315772"}},"outputId":"f36fbe12-b425-4f2d-b228-19475d6ff0be"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Failed to solve puzzle 107 with model gemini-1.5-pro\n","Failed to solve puzzle 128 with model gemini-1.5-pro\n","Failed to solve puzzle 107 with model gemini-1.5-flash\n","Failed to solve puzzle 128 with model gemini-1.5-flash\n"]},{"output_type":"execute_result","data":{"text/plain":["{'test_size': 200,\n"," 'gemini-1.5-pro': 34,\n"," 'gemini-1.5-flash': 17,\n"," 'gpt-4o-2024-08-06': 59,\n"," 'gpt-4o-mini': 20}"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["# from google.colab import runtime\n","# runtime.unassign()\n"],"metadata":{"id":"EZsovBI40mJD","executionInfo":{"status":"ok","timestamp":1726322548215,"user_tz":-180,"elapsed":1,"user":{"displayName":"Sam Yiin","userId":"15309663941607315772"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["# Legacy"],"metadata":{"id":"Y8knHdhxfWkU"}},{"cell_type":"markdown","source":["backup for the cacehd code"],"metadata":{"id":"X28xUqIRmeH1"}},{"cell_type":"code","source":["# # Carefullll Do not change this code!!!!\n","\n","# from joblib import Memory\n","\n","# # Work around for joblib caching in jupyter notebook \"joblib persistence across sessions/machines\"\n","# def cache(mem, module, **mem_kwargs):\n","#     # model is the notebook/python file name: Jupyter notebook's name is always changing so we need this work around\n","#     def cache_(f):\n","#         f.__module__ = module\n","#         f.__qualname__ = f.__name__\n","#         return mem.cache(f, **mem_kwargs)\n","#     # return the cache function that will always create same name for cahce directory\n","#     return cache_\n","\n","# # Create a memory object with a cache directory\n","# memory = Memory(location=\"PromptEngineering/FunctionCache\", verbose=0)\n","\n","# @cache(memory, \"AutoStrategySelection\")\n","# def solve_puzzle(sample, model, attempt=1):\n","#     if \"gemini\" in model:\n","#         chat_bot, information_extractor = load_gemini_chat_bot(model)\n","#     else:\n","#         chat_bot, information_extractor = load_gpt_chat_bot(model)\n","#     prompt = f\"**Clue**: {sample['clue']}\\n**Orientation**: {sample['orientation']}\"\n","#     response = chat_bot.chat(prompt)\n","\n","#     # extract information\n","#     prompt_extract = f\"Given the output:\\n{response}, What is the answer? I don't need other information.\"\n","#     response_extract = information_extractor.chat(prompt_extract)\n","#     return response, response_extract"],"metadata":{"id":"C3aXR92tmcfn","executionInfo":{"status":"ok","timestamp":1726322548216,"user_tz":-180,"elapsed":2,"user":{"displayName":"Sam Yiin","userId":"15309663941607315772"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# import os\n","# import json\n","# import shutil\n","# import ast\n","\n","# # Set the cache directory where joblib stores the cached files\n","# cache_dir = \"PromptEngineering/FunctionCache/joblib/AutoStrategySelection/solve_puzzle/\"\n","\n","\n","# # Set of answers that you want to delete from the cache\n","# answers_to_delete = {\"broad minded\", \"flood water\", \"amiss\", 'up the pole', 'dog ends', 'thrashngg', 'baldpate', 'unseemly', 'gnu', 'refused'}\n","# gemini_flash = \"gemini-1.5-flash\"\n","\n","# # Traverse the cache directory\n","# for root, dirs, files in os.walk(cache_dir):\n","#     if 'metadata.json' in files:\n","#         metadata_path = os.path.join(root, 'metadata.json')\n","\n","#         # Open and read the metadata.json file\n","#         with open(metadata_path, 'r') as f:\n","#             metadata = json.load(f)\n","\n","#         # Check if the answer field matches the ones we want to delete\n","#         try:\n","#             answer = ast.literal_eval(metadata['input_args']['sample'])['answer']\n","#             model = ast.literal_eval(metadata['input_args']['model'])\n","#             if (answer.strip() in answers_to_delete) and model == gemini_flash:\n","#                 print(f\"Found match: {answer}, in directory: {root}\")\n","\n","#                 # Record and delete the directory\n","#                 shutil.rmtree(root)\n","#                 print(f\"Deleted cache directory: {root}\")\n","#         except KeyError:\n","#             # If the expected structure isn't found, skip this directory\n","#             print(f\"No matching 'answer' field found in {metadata_path}\")\n","\n","# print(\"Completed cache cleanup.\")\n"],"metadata":{"id":"YJoHpYhedLV6","executionInfo":{"status":"ok","timestamp":1726322548684,"user_tz":-180,"elapsed":6,"user":{"displayName":"Sam Yiin","userId":"15309663941607315772"}}},"execution_count":11,"outputs":[]}]}