{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5231416c-d47b-4007-913b-d5f5fcde6858",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Roberta Cryptonite\n",
    "We will try to use Roberta to predict the next word of Cryptonite. The plan is that first, we will use RoBerta to predict the answer. Then we may try to use masked output, and then we may integrated some prompt engineering techniques like chain of thoughs.   \n",
    "Here are the link to implementation:  \n",
    "[https://huggingface.co/docs/transformers/en/model_doc/roberta](https://huggingface.co/docs/transformers/en/model_doc/roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5447eccd-c240-4625-92e2-5ebf254da0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_train_fp = '../datasets/cryptonite-official-split/cryptonite-train-choice.jsonl'\n",
    "modified_val_fp = \"../datasets/cryptonite-official-split/cryptonite-val-choice.jsonl\"\n",
    "modified_test_fp = '../datasets/cryptonite-official-split/cryptonite-test-choice.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caa2c13e-c244-42ca-aa3b-70af69e290d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c9f15dcaa5a42c0a068ef94b5338519",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aef7b917be0645e4bd1d0fd99b35acb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f7de2d4cdbd4a30a3ac49e82d2247e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# !pip install datasets\n",
    "from datasets import load_dataset\n",
    "# dataset = load_dataset(\"aviaefrat/cryptonite\", \"cryptonite\")\n",
    "dataset = load_dataset('json', data_files={'train': modified_train_fp, 'validation': modified_val_fp, 'test': modified_test_fp})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6c1c219-cbaa-4ffc-88f8-5da8f935ef1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'publisher': 'Times',\n",
       " 'date': 971913600000,\n",
       " 'author': '',\n",
       " 'number': '1',\n",
       " 'orientation': 'across',\n",
       " 'clue': 'got rid of piece of hi-fi, a particular horror of mine (8)',\n",
       " 'answer': 'firedamp',\n",
       " 'enumeration': '(8)',\n",
       " 'quick': False,\n",
       " 'sub_publisher': 'The Times',\n",
       " 'choice1': 'Aktivist',\n",
       " 'choice2': 'rentaler',\n",
       " 'choice3': 'seemably'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install transformers\n",
    "# !pip install torch torchvision\n",
    "dataset['train'][77]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bd3daf-cb2a-4aa0-99a7-cc3d2afba089",
   "metadata": {},
   "source": [
    "## Multiple Choice Roberta (Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c1c6e8e-4d0d-4eca-a028-8022bad2aa0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForMultipleChoice were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, RobertaForMultipleChoice\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "model = RobertaForMultipleChoice.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "\n",
    "# take a random example from train set\n",
    "sample = dataset['train'][77]\n",
    "\n",
    "prompt = sample['clue']\n",
    "choice1 = sample['answer']\n",
    "choice0 = sample['choice1']\n",
    "choice2 = sample['choice2']\n",
    "choice3 = sample['choice3']\n",
    "\n",
    "prompts = [prompt, prompt, prompt, prompt]\n",
    "choices = [choice0, choice1, choice2, choice3]\n",
    "# choice0 is correct (according to Wikipedia ;)), batch size 1\n",
    "labels = torch.tensor(1).unsqueeze(0)  \n",
    "# Each instance of the prompt corresponds to a different choice. so [prompt, prompt], [choice0, choice1]\n",
    "encoding = tokenizer(prompts, choices, return_tensors=\"pt\", padding=True)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**{k: v.unsqueeze(0) for k, v in encoding.items()}, labels=labels)  # batch size is 1\n",
    "\n",
    "# the linear classifier still needs to be trained\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66b6957e-55f9-4eaf-9012-b895faa166c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0078, -0.0090, -0.0136, -0.0077]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5d4144-64d0-411b-8836-e0e623570079",
   "metadata": {},
   "source": [
    "Let's test the accuracy on the first 100 samples. Assuming higher logits correspond to higher probabilities assigned by the model to the corresponding class or choice. I will write some ugly code here...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bb46d20-30ad-423f-a661-968a7671a14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_correct_count = 0\n",
    "total_count = 0\n",
    "for i in range(1000):\n",
    "    # take a random example from train set\n",
    "    sample = dataset['train'][i]\n",
    "    \n",
    "    prompt = sample['clue']\n",
    "    choice0 = sample['answer']\n",
    "    choice1 = sample['choice1']\n",
    "    choice2 = sample['choice2']\n",
    "    choice3 = sample['choice3']\n",
    "    \n",
    "    prompts = [prompt, prompt, prompt, prompt]\n",
    "    choices = [choice0, choice1, choice2, choice3]\n",
    "    # choice0 is correct (according to Wikipedia ;)), batch size 1\n",
    "    labels = torch.tensor(0).unsqueeze(0)  \n",
    "    # Each instance of the prompt corresponds to a different choice. so [prompt, prompt], [choice0, choice1]\n",
    "    encoding = tokenizer(prompts, choices, return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**{k: v.unsqueeze(0) for k, v in encoding.items()}, labels=labels)  # batch size is 1\n",
    "    \n",
    "    # the linear classifier still needs to be trained\n",
    "    loss = outputs.loss\n",
    "    logits = outputs.logits[0]\n",
    "\n",
    "    # see if the first choice is the highest probability\n",
    "    if logits[0] > torch.max(logits[1:]):\n",
    "        predict_correct_count += 1\n",
    "    total_count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1322de63-2c77-4c88-aada-7a270f4b226d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct predict 198 out of 1000, accuracy: 0.198\n"
     ]
    }
   ],
   "source": [
    "print(f'correct predict {predict_correct_count} out of {total_count}, accuracy: {predict_correct_count/total_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0ac9cc-4151-4422-9bc6-18c891e29e95",
   "metadata": {},
   "source": [
    "So without training, the success rate is less than 25% (random guess)....... The next thing we can do is training on small dataset and see if there is improvement is loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6261790-4f1b-4654-8298-68db39ec4ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b34e4e3-9eb0-40e8-bf22-4128d3b2f537",
   "metadata": {},
   "source": [
    "## Multiple Choice Roberta (Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aa0a5322-fdf1-4baa-8ea0-aba8dcf45081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# preprocess the current dataset all together\n",
    "def preprocess_function(sample):\n",
    "    prompt = sample['clue']\n",
    "    choice0 = sample['answer']\n",
    "    choice1 = sample['choice1']\n",
    "    choice2 = sample['choice2']\n",
    "    choice3 = sample['choice3']\n",
    "    \n",
    "    prompts = [prompt, prompt, prompt, prompt]\n",
    "    choices = [choice0, choice1, choice2, choice3]\n",
    "  \n",
    "    # Each instance of the prompt corresponds to a different choice. so [prompt, prompt], [choice0, choice1]\n",
    "    encoding = tokenizer(prompts, choices, return_tensors=\"pt\", padding=True)\n",
    "    # choice0 is correct (according to Wikipedia ;)), batch size 1\n",
    "    labels = torch.tensor(0).unsqueeze(0) \n",
    "    # add labels to encoding\n",
    "    encoding['labels'] = labels\n",
    "    return encoding\n",
    "\n",
    "processed_dataset_dir = '../datasets/processed_dataset'\n",
    "if not os.path.exists(processed_dataset_dir):\n",
    "    dataset.set_format(columns=['clue', 'answer', 'choice1', 'choice2', 'choice3'])\n",
    "    processed_dataset = dataset.map(preprocess_function) # take about 4 mins\n",
    "    # save the processed dataset so that we don't need to process again\n",
    "    processed_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    processed_dataset.save_to_disk(processed_dataset_dir)\n",
    "processed_dataset = load_dataset(processed_dataset_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045a8ee0-23f4-4673-9d34-dd893d983922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model defined above: model = RobertaForMultipleChoice.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "# define hyper parameters\n",
    "lr = 0.001\n",
    "# define optimizer and criteria\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfe4976-cec3-470a-a802-c951e8fb128c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Masked Roberta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a167a9f2-26c6-488d-ace5-d9820992fcfd",
   "metadata": {},
   "source": [
    "What can we do with mask? I feel like a lot of stuff, including chain of thoughs. And training, like we can train simple things like  \"The word *mask* is nine letters long.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "cb538de4-dc42-4442-ac40-ab11b6f66f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, RobertaForMaskedLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "model = RobertaForMaskedLM.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "\n",
    "inputs = tokenizer(\"The word '<mask>' is nine letters long.\", return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # the output before sigmoid/softmax: we need argmax, so we don't need soft max here...\n",
    "    logits = model(**inputs).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a1fbc13d-671f-4c24-a8e1-6004585d42ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 50265])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931d3b10-bbfa-4f26-957a-b8f955c80ac7",
   "metadata": {},
   "source": [
    "From [https://huggingface.co/docs/transformers/v4.41.3/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput](https://huggingface.co/docs/transformers/v4.41.3/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput)\n",
    "logits: (torch.FloatTensor of shape (batch_size, sequence_length, config.vocab_size)) â€” Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)  \n",
    "batch_size = 1 beacause we are sending in one sentence.\n",
    "sequence_length is the length of tokens (not words)\n",
    "The last one is vocab size, so for each token, it is giving the probability of each words (50k here), and we just want the probability of the masked word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fd0a84a0-7b94-402c-bd9f-2e67fef2df8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fuck'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieve index of <mask>: there might be more than one, so mask_token_index is a list.\n",
    "mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "# this is taking the most likely answer, we can let it return the top 10 answers\n",
    "predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
    "tokenizer.decode(predicted_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "00160af7-a60c-4a76-8d44-b6471e19369f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fuck            0.0117678      \n",
      "no              0.0100174      \n",
      "I               0.00918356     \n",
      "a               0.00591128     \n",
      "mother          0.00542913     \n",
      "we              0.00518939     \n",
      "it              0.00500547     \n",
      "love            0.00497648     \n",
      "you             0.00490687     \n",
      "home            0.00414038     \n"
     ]
    }
   ],
   "source": [
    "# above is taking the most likely answer, we can let it return the top 10 answers too\n",
    "masked_token_logits = logits[0, mask_token_index, :]\n",
    "# we could also calculate the probability of each (apply softmax on the logit - get probability of each word)\n",
    "probabilities = torch.softmax(masked_token_logits, dim=-1)[0]\n",
    "\n",
    "k = 10\n",
    "# torch.topk returns top k values, so .indices returns top k values' indices\n",
    "top_k_tokens = torch.topk(masked_token_logits, k, dim=1).indices[0].tolist()\n",
    "for token_idx in top_k_tokens:\n",
    "    predicted_word = tokenizer.decode(token_idx)\n",
    "    word_prob = probabilities[token_idx]\n",
    "    print(f\"{predicted_word:<15} {word_prob:<15.6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1b015d-19ce-4357-9428-633f91b6574c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
